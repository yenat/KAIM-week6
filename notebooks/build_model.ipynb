{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer, OrdinalEncoder\n",
    "\n",
    "# Load your data from the CSV file\n",
    "data = pd.read_csv('feature_engineered_data.csv')\n",
    "\n",
    "# Assuming recency, frequency, monetary, and seniority columns are derived from your existing columns\n",
    "data['recency'] = np.random.randint(1, 100, size=len(data))  # Example logic\n",
    "data['frequency'] = data['TransactionCount']\n",
    "data['monetary'] = data['TotalTransactionAmount']\n",
    "data['seniority'] = 2025 - data['TransactionYear']\n",
    "\n",
    "data['rfms_score'] = data['recency'] * 0.2 + data['frequency'] * 0.3 + data['monetary'] * 0.4 + data['seniority'] * 0.1\n",
    "\n",
    "# Establish a boundary for classification\n",
    "boundary = data['rfms_score'].median()\n",
    "\n",
    "# Assign good and bad labels\n",
    "data['label'] = np.where(data['rfms_score'] > boundary, 'good', 'bad')\n",
    "\n",
    "\n",
    "# Function to calculate Weight of Evidence (WoE)\n",
    "def calc_woe(df, feature, target):\n",
    "    df_woe = df.groupby(feature)[target].agg(['count', 'sum'])\n",
    "    df_woe.columns = ['n', 'bad']\n",
    "    df_woe['good'] = df_woe['n'] - df_woe['bad']\n",
    "    df_woe['bad_prop'] = df_woe['bad'] / df_woe['bad'].sum()\n",
    "    df_woe['good_prop'] = df_woe['good'] / df_woe['good'].sum()\n",
    "    df_woe['woe'] = np.log(df_woe['good_prop'] / df_woe['bad_prop'])\n",
    "    return df_woe['woe']\n",
    "\n",
    "# Discretize the features into bins\n",
    "est = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "data_binned = est.fit_transform(data[['recency', 'frequency', 'monetary', 'seniority']])\n",
    "data_binned = pd.DataFrame(data_binned, columns=['recency_bin', 'frequency_bin', 'monetary_bin', 'seniority_bin'])\n",
    "\n",
    "# Calculate WoE for each binned feature\n",
    "default_column = 'FraudResult'  # Adjust the column name if needed\n",
    "for feature in ['recency_bin', 'frequency_bin', 'monetary_bin', 'seniority_bin']:\n",
    "    data_binned[f'{feature}_woe'] = calc_woe(data_binned.join(data[[default_column]]), feature, default_column)\n",
    "\n",
    "# Combine the original and WoE-transformed features\n",
    "data_combined = pd.concat([data, data_binned[['recency_bin_woe', 'frequency_bin_woe', 'monetary_bin_woe', 'seniority_bin_woe']]], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'TransactionStartTime' column exists\n",
    "if 'TransactionStartTime' in data_combined.columns:\n",
    "    # Convert `TransactionStartTime` to datetime format\n",
    "    data_combined['TransactionStartTime'] = pd.to_datetime(data_combined['TransactionStartTime'])\n",
    "\n",
    "    # Extract features from `TransactionStartTime`\n",
    "    data_combined['TransactionHour'] = data_combined['TransactionStartTime'].dt.hour\n",
    "    data_combined['TransactionDay'] = data_combined['TransactionStartTime'].dt.day\n",
    "    data_combined['TransactionMonth'] = data_combined['TransactionStartTime'].dt.month\n",
    "    data_combined['TransactionYear'] = data_combined['TransactionStartTime'].dt.year\n",
    "\n",
    "    # Drop the original `TransactionStartTime` column\n",
    "    data_combined.drop('TransactionStartTime', axis=1, inplace=True)\n",
    "else:\n",
    "    print(\"The 'TransactionStartTime' column is not present in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId',\n",
      "       'CurrencyCode', 'CountryCode', 'ProviderId', 'ProductId',\n",
      "       'ProductCategory', 'ChannelId', 'Amount', 'Value', 'PricingStrategy',\n",
      "       'FraudResult', 'TransactionHour', 'TransactionDay', 'TransactionMonth',\n",
      "       'TransactionYear', 'ChannelIdEncoded', 'ProductCategory_airtime',\n",
      "       'ProductCategory_data_bundles', 'ProductCategory_financial_services',\n",
      "       'ProductCategory_movies', 'ProductCategory_other',\n",
      "       'ProductCategory_ticket', 'ProductCategory_transport',\n",
      "       'ProductCategory_tv', 'ProductCategory_utility_bill',\n",
      "       'TotalTransactionAmount', 'AvgTransactionAmount', 'TransactionCount',\n",
      "       'StdDevTransactionAmount', 'recency', 'frequency', 'monetary',\n",
      "       'seniority', 'rfms_score', 'label', 'recency_bin_woe',\n",
      "       'frequency_bin_woe', 'monetary_bin_woe', 'seniority_bin_woe'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical features\n",
    "categorical_features = ['CurrencyCode', 'CountryCode', 'ProviderId', 'ProductId', 'ProductCategory', 'ChannelId', 'PricingStrategy']\n",
    "\n",
    "# Apply ordinal encoding to categorical features\n",
    "encoder = OrdinalEncoder()\n",
    "data_combined[categorical_features] = encoder.fit_transform(data_combined[categorical_features])\n",
    "\n",
    "# Display columns for verification\n",
    "print(data_combined.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define the features (X) and the target variable (y)\n",
    "X = data_combined.drop(['label', 'TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId'], axis=1)\n",
    "y = data_combined['label']\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Initialize the SimpleImputer with median strategy\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Apply the imputer to the feature set\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Convert the result back to a DataFrame\n",
    "X = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (61223, 37)\n",
      "Testing set size: (19133, 37)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Make sure these steps only apply to training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Training set size: {X_train.shape}')\n",
    "print(f'Testing set size: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations only to training data\n",
    "# For example: WOE or scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the SimpleImputer with median strategy\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Apply the imputer only to the training set\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Apply scaling to the training set and then apply the same scaler to validation and test sets\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_val_scaled = scaler.transform(X_val_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple Logistic Regression Performance:\n",
      "Accuracy: 0.8820723899124526\n",
      "Precision: 0.9585927770859277\n",
      "Recall: 0.8000519682993374\n",
      "F1 Score: 0.872176191487855\n",
      "ROC-AUC: 0.9708676839733886\n",
      "Predictions saved to 'simple_model_predictions_with_features.csv'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a simpler Logistic Regression model\n",
    "log_reg_simple = LogisticRegression(C=0.01, penalty='l2', max_iter=1000)\n",
    "log_reg_simple.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_log_reg_simple = log_reg_simple.predict(X_val_scaled)\n",
    "\n",
    "# Compute evaluation metrics for Logistic Regression\n",
    "accuracy_log_reg_simple = accuracy_score(y_val, y_pred_log_reg_simple)\n",
    "precision_log_reg_simple = precision_score(y_val, y_pred_log_reg_simple, pos_label='good')\n",
    "recall_log_reg_simple = recall_score(y_val, y_pred_log_reg_simple, pos_label='good')\n",
    "f1_log_reg_simple = f1_score(y_val, y_pred_log_reg_simple, pos_label='good')\n",
    "roc_auc_log_reg_simple = roc_auc_score(y_val, log_reg_simple.predict_proba(X_val_scaled)[:, 1])\n",
    "\n",
    "# Display evaluation results\n",
    "print(\"\\nSimple Logistic Regression Performance:\")\n",
    "print(f\"Accuracy: {accuracy_log_reg_simple}\")\n",
    "print(f\"Precision: {precision_log_reg_simple}\")\n",
    "print(f\"Recall: {recall_log_reg_simple}\")\n",
    "print(f\"F1 Score: {f1_log_reg_simple}\")\n",
    "print(f\"ROC-AUC: {roc_auc_log_reg_simple}\")\n",
    "\n",
    "# Combine X_val with the actual and predicted labels\n",
    "results = pd.DataFrame(X_val_scaled, columns=X.columns)\n",
    "results['Actual'] = y_val\n",
    "results['LogReg_Predicted'] = y_pred_log_reg_simple\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results.to_csv('simple_model_predictions_with_features.csv', index=False)\n",
    "print(\"Predictions saved to 'simple_model_predictions_with_features.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple Logistic Regression Performance:\n",
      "Accuracy: 0.8815497190644191\n",
      "Precision: 0.9566904687985098\n",
      "Recall: 0.800701572041055\n",
      "F1 Score: 0.8717731098380367\n",
      "ROC-AUC: 0.9705720711575034\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "Best Random Forest (with Grid Search) Performance:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "ROC-AUC: 1.0\n",
      "Predictions saved to 'model_predictions_with_features.csv'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_val.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Initialize the SimpleImputer with median strategy\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Apply the imputer only to the training set\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Apply scaling to the training set and then apply the same scaler to validation and test sets\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_val_scaled = scaler.transform(X_val_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Initialize and train a simpler Logistic Regression model\n",
    "log_reg_simple = LogisticRegression(C=0.01, penalty='l2', max_iter=1000)\n",
    "log_reg_simple.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set for Logistic Regression\n",
    "y_pred_log_reg_simple = log_reg_simple.predict(X_val_scaled)\n",
    "\n",
    "# Compute evaluation metrics for Logistic Regression\n",
    "accuracy_log_reg_simple = accuracy_score(y_val, y_pred_log_reg_simple)\n",
    "precision_log_reg_simple = precision_score(y_val, y_pred_log_reg_simple, pos_label='good')\n",
    "recall_log_reg_simple = recall_score(y_val, y_pred_log_reg_simple, pos_label='good')\n",
    "f1_log_reg_simple = f1_score(y_val, y_pred_log_reg_simple, pos_label='good')\n",
    "roc_auc_log_reg_simple = roc_auc_score(y_val, log_reg_simple.predict_proba(X_val_scaled)[:, 1])\n",
    "\n",
    "# Display evaluation results for Logistic Regression\n",
    "print(\"\\nSimple Logistic Regression Performance:\")\n",
    "print(f\"Accuracy: {accuracy_log_reg_simple}\")\n",
    "print(f\"Precision: {precision_log_reg_simple}\")\n",
    "print(f\"Recall: {recall_log_reg_simple}\")\n",
    "print(f\"F1 Score: {f1_log_reg_simple}\")\n",
    "print(f\"ROC-AUC: {roc_auc_log_reg_simple}\")\n",
    "\n",
    "# Initialize and train a simpler Random Forest model with Grid Search for Hyperparameter Tuning\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [10, 20],\n",
    "    'min_samples_leaf': [10, 20]\n",
    "}\n",
    "grid_search_rf = GridSearchCV(RandomForestClassifier(), rf_params, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "# Predict on the validation set for Random Forest\n",
    "y_pred_best_rf = best_rf.predict(X_val_scaled)\n",
    "\n",
    "# Compute evaluation metrics for Best Random Forest\n",
    "accuracy_best_rf = accuracy_score(y_val, y_pred_best_rf)\n",
    "precision_best_rf = precision_score(y_val, y_pred_best_rf, pos_label='good')\n",
    "recall_best_rf = recall_score(y_val, y_pred_best_rf, pos_label='good')\n",
    "f1_best_rf = f1_score(y_val, y_pred_best_rf, pos_label='good')\n",
    "roc_auc_best_rf = roc_auc_score(y_val, best_rf.predict_proba(X_val_scaled)[:, 1])\n",
    "\n",
    "# Display evaluation results for Random Forest\n",
    "print(\"\\nBest Random Forest (with Grid Search) Performance:\")\n",
    "print(f\"Accuracy: {accuracy_best_rf}\")\n",
    "print(f\"Precision: {precision_best_rf}\")\n",
    "print(f\"Recall: {recall_best_rf}\")\n",
    "print(f\"F1 Score: {f1_best_rf}\")\n",
    "print(f\"ROC-AUC: {roc_auc_best_rf}\")\n",
    "\n",
    "# Combine X_val with the actual and predicted labels\n",
    "results = pd.DataFrame(X_val_scaled, columns=X.columns)\n",
    "results['Actual'] = y_val\n",
    "results['LogReg_Predicted'] = y_pred_log_reg_simple\n",
    "results['RandomForest_Predicted'] = y_pred_best_rf\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results.to_csv('model_predictions_with_features.csv', index=False)\n",
    "print(\"Predictions saved to 'model_predictions_with_features.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
